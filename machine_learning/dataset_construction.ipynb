{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Construction\n",
    "save the masks and images as .png in the following folder structure:\n",
    "\n",
    "```\n",
    "dataset/   #Primary data folder for the project\n",
    "├── input/           #All input data is stored here. \n",
    "│   ├── train_images/\n",
    "│   │   ├── image01.png\n",
    "│   │   ├── image02.png\n",
    "│   │   └── ...\n",
    "│   ├── train_masks/        #All binary masks organized in respective sub-directories.\n",
    "│   │   ├── class1/\n",
    "│   │   │   ├── image01.png\n",
    "│   │   │   ├── image02.png\n",
    "│   │   │   └── ...\n",
    "│   │   ├── class2/\n",
    "│   │   │   ├── image01.png\n",
    "│   │   │   ├── image02.png\n",
    "│   │   │   └── ...\n",
    "│   ├── val_images/         #Validation images\n",
    "│   │   ├── image01.png\n",
    "│   │   ├── image02.png\n",
    "│   │   └── ...\n",
    "│   ├── val_masks/          #Validation masks organized in respective sub-directories.\n",
    "│   │   ├── class1/\n",
    "│   │   │   ├── image01.png\n",
    "│   │   │   ├── image02.png\n",
    "│   │   │   └── ...\n",
    "│   │   ├── class2/\n",
    "│   │   │   ├── image01.png\n",
    "│   │   │   ├── image02.png\n",
    "│   │   │   └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "train_mask_path = os.path.join(\"dataset\", \"input\", \"train_masks\")\n",
    "train_json_path = os.path.join(\"dataset\", \"input\", \"train_images\", \"train.json\")\n",
    "val_mask_path = os.path.join(\"dataset\", \"input\", \"val_masks\")\n",
    "val_json_path = os.path.join(\"dataset\", \"input\", \"val_images\", \"val.json\")\n",
    "train_mask_path = os.path.join(os.getcwd(), train_mask_path)\n",
    "train_json_path = os.path.join(os.getcwd(), train_json_path)\n",
    "val_mask_path = os.path.join(os.getcwd(), val_mask_path)\n",
    "val_json_path = os.path.join(os.getcwd(), val_json_path)\n",
    "\n",
    "\n",
    "# Label IDs of the dataset representing different categories\n",
    "category_ids = {\n",
    "    \"class1\": 1, #make sure the name of the class is the same as the folder name in the dataset \n",
    "}\n",
    "\n",
    "MASK_EXT = 'png'\n",
    "ORIGINAL_EXT = 'png'\n",
    "image_id = 0\n",
    "annotation_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def images_annotations_info(maskpath):\n",
    "    \"\"\"\n",
    "    Process the binary masks and generate images and annotations information.\n",
    "\n",
    "    :param maskpath: Path to the directory containing binary masks\n",
    "    :return: Tuple containing images info, annotations info, and annotation count\n",
    "    \"\"\"\n",
    "    global image_id, annotation_id\n",
    "    annotations = []\n",
    "    images = []\n",
    "\n",
    "    # Iterate through categories and corresponding masks\n",
    "    for category in category_ids.keys():\n",
    "        for mask_image in glob.glob(os.path.join(maskpath, category, f'*.{MASK_EXT}')):\n",
    "            original_file_name = f'{os.path.basename(mask_image).split(\".\")[0]}.{ORIGINAL_EXT}'\n",
    "            mask_image_open = cv2.imread(mask_image)\n",
    "            \n",
    "            # Get image dimensions\n",
    "            height, width, _ = mask_image_open.shape\n",
    "\n",
    "            # Create or find existing image annotation\n",
    "            if original_file_name not in map(lambda img: img['file_name'], images):\n",
    "                image = {\n",
    "                    \"id\": image_id + 1,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height,\n",
    "                    \"file_name\": original_file_name,\n",
    "                }\n",
    "                images.append(image)\n",
    "                image_id += 1\n",
    "            else:\n",
    "                image = [element for element in images if element['file_name'] == original_file_name][0]\n",
    "\n",
    "            # Find contours in the mask image\n",
    "            gray = cv2.cvtColor(mask_image_open, cv2.COLOR_BGR2GRAY)\n",
    "            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            contours = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "            # Create annotation for each contour\n",
    "            for contour in contours:\n",
    "                bbox = cv2.boundingRect(contour)\n",
    "                area = cv2.contourArea(contour)\n",
    "                segmentation = contour.flatten().tolist()\n",
    "\n",
    "                annotation = {\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"id\": annotation_id,\n",
    "                    \"image_id\": image['id'],\n",
    "                    \"category_id\": category_ids[category],\n",
    "                    \"bbox\": bbox,\n",
    "                    \"area\": area,\n",
    "                    \"segmentation\": [segmentation],\n",
    "                }\n",
    "\n",
    "                # Add annotation if area is greater than zero\n",
    "                if area > 0:\n",
    "                    annotations.append(annotation)\n",
    "                    annotation_id += 1\n",
    "\n",
    "    return images, annotations, annotation_id\n",
    "\n",
    "\n",
    "def process_masks(mask_path, dest_json):\n",
    "    global image_id, annotation_id\n",
    "    image_id = 0\n",
    "    annotation_id = 0\n",
    "\n",
    "    # Initialize the COCO JSON format with categories\n",
    "    coco_format = {\n",
    "        \"info\": {},\n",
    "        \"licenses\": [],\n",
    "        \"images\": [],\n",
    "        \"categories\": [{\"id\": value, \"name\": key, \"supercategory\": key} for key, value in category_ids.items()],\n",
    "        \"annotations\": [],\n",
    "    }\n",
    "\n",
    "    # Create images and annotations sections\n",
    "    coco_format[\"images\"], coco_format[\"annotations\"], annotation_cnt = images_annotations_info(mask_path)\n",
    "\n",
    "    # Save the COCO JSON to a file\n",
    "    print(\"Saving annotations to file: %s\" % dest_json)\n",
    "    with open(dest_json, \"w\") as outfile:\n",
    "        json.dump(coco_format, outfile, sort_keys=True, indent=4)\n",
    "\n",
    "    print(\"Created %d annotations for images in folder: %s\" % (annotation_cnt, mask_path))\n",
    "\n",
    "process_masks(train_mask_path, train_json_path)\n",
    "process_masks(val_mask_path, val_json_path)\n",
    "\n",
    "# test_mask_path = \"\"\n",
    "# test_json_path = \"\"\n",
    "# process_masks(test_mask_path, test_json_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_input_path = os.path.join(\"dataset\", \"input\")\n",
    "base_input_path = os.path.join(os.getcwd(), base_input_path)\n",
    "\n",
    "base_output_path = os.path.join(\"dataset\", \"yolo_dataset\")\n",
    "base_output_path = os.path.join(os.getcwd(), base_output_path)\n",
    "\n",
    "#where the yaml file will be saved\n",
    "train_path = os.path.join(\"dataset\", \"yolo_dataset\", \"train\", \"images\")\n",
    "train_path = os.path.join(os.getcwd(), train_path)\n",
    "val_path = os.path.join(\"dataset\", \"yolo_dataset\", \"valid\", \"images\")\n",
    "val_path = os.path.join(os.getcwd(), val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "# Function to convert images to YOLO format\n",
    "def convert_to_yolo(input_images_path, input_json_path, output_images_path, output_labels_path):\n",
    "    # Open JSON file containing image annotations\n",
    "    f = open(input_json_path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    # Create directories for output images and labels\n",
    "    os.makedirs(output_images_path, exist_ok=True)\n",
    "    os.makedirs(output_labels_path, exist_ok=True)\n",
    "\n",
    "    # List to store filenames\n",
    "    file_names = []\n",
    "    for filename in os.listdir(input_images_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            source = os.path.join(input_images_path, filename)\n",
    "            destination = os.path.join(output_images_path, filename)\n",
    "            shutil.copy(source, destination)\n",
    "            file_names.append(filename)\n",
    "\n",
    "    # Function to get image annotations\n",
    "    def get_img_ann(image_id):\n",
    "        return [ann for ann in data['annotations'] if ann['image_id'] == image_id]\n",
    "\n",
    "    # Function to get image data\n",
    "    def get_img(filename):\n",
    "        return next((img for img in data['images'] if img['file_name'] == filename), None)\n",
    "\n",
    "    # Iterate through filenames and process each image\n",
    "    for filename in file_names:\n",
    "        img = get_img(filename)\n",
    "        img_id = img['id']\n",
    "        img_w = img['width']\n",
    "        img_h = img['height']\n",
    "        img_ann = get_img_ann(img_id)\n",
    "\n",
    "        # Write normalized polygon data to a text file\n",
    "        if img_ann:\n",
    "            with open(os.path.join(output_labels_path, f\"{os.path.splitext(filename)[0]}.txt\"), \"a\") as file_object:\n",
    "                for ann in img_ann:\n",
    "                    current_category = ann['category_id'] - 1\n",
    "                    polygon = ann['segmentation'][0]\n",
    "                    normalized_polygon = [format(coord / img_w if i % 2 == 0 else coord / img_h, '.6f') for i, coord in enumerate(polygon)]\n",
    "                    file_object.write(f\"{current_category} \" + \" \".join(normalized_polygon) + \"\\n\")\n",
    "\n",
    "# Function to create a YAML file for the dataset\n",
    "def create_yaml(input_json_path, output_yaml_path, train_path, val_path, test_path=None):\n",
    "    with open(input_json_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract the category names\n",
    "    names = [category['name'] for category in data['categories']]\n",
    "    \n",
    "    # Number of classes\n",
    "    nc = len(names)\n",
    "\n",
    "    # Create a dictionary with the required content\n",
    "    yaml_data = {\n",
    "        'names': names,\n",
    "        'nc': nc,\n",
    "        'test': test_path if test_path else '',\n",
    "        'train': train_path,\n",
    "        'val': val_path\n",
    "    }\n",
    "\n",
    "    # Write the dictionary to a YAML file\n",
    "    with open(output_yaml_path, 'w') as file:\n",
    "        yaml.dump(yaml_data, file, default_flow_style=False)\n",
    "\n",
    "\n",
    "# Processing validation dataset (if needed)\n",
    "convert_to_yolo(\n",
    "    input_images_path=os.path.join(base_input_path, \"val_images\"),\n",
    "    input_json_path=os.path.join(base_input_path, \"val_images/val.json\"),\n",
    "    output_images_path=os.path.join(base_output_path, \"valid/images\"),\n",
    "    output_labels_path=os.path.join(base_output_path, \"valid/labels\")\n",
    ")\n",
    "\n",
    "# Processing training dataset \n",
    "convert_to_yolo(\n",
    "    input_images_path=os.path.join(base_input_path, \"train_images\"),\n",
    "    input_json_path=os.path.join(base_input_path, \"train_images/train.json\"),\n",
    "    output_images_path=os.path.join(base_output_path, \"train/images\"),\n",
    "    output_labels_path=os.path.join(base_output_path, \"train/labels\")\n",
    ")\n",
    "\n",
    "# Creating the YAML configuration file\n",
    "create_yaml(\n",
    "    input_json_path=os.path.join(base_input_path, \"train_images/train.json\"),\n",
    "    output_yaml_path=os.path.join(base_output_path, \"data.yaml\"),\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    # test_path='../test/images'  # or None if not applicable\n",
    "    test_path=None  # or None if not applicable\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
